{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGloitLPceFzkXFAXPMAAQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhvis29/Hands_On_LLM_WR/blob/main/Ch8_Semantic_Search_and_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxliyrakrIIO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain==0.2.5 faiss-cpu==1.8.0 cohere==5.5.8 langchain-community==0.2.5 rank_bm25==0.2.2 sentence-transformers==3.0.1\n",
        "!pip install llama-cpp-python==0.2.78  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "id": "2k3pxbywrO1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "OEs300rjH_rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade --force-reinstall langchain"
      ],
      "metadata": {
        "id": "FDrK9UYUaiqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install --upgrade --force-reinstall numpy pandas"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jfI9L8H1cLju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types"
      ],
      "metadata": {
        "id": "dRvTry5YrdyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "XIrh8c4qMkF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "kpWuZqWorfa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "ynC658j6stpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini_llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "n7uwHTbYrm5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "4B6mfz6ysp4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  if 'embedContent' in model.supported_actions:\n",
        "    print(model.name)"
      ],
      "metadata": {
        "id": "GF1cv2xbs6Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dense Retrieval Example"
      ],
      "metadata": {
        "id": "CJxf3XO2A9u9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Getting the text archive and chunking it"
      ],
      "metadata": {
        "id": "ZgyLcTRDA-_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "\n",
        "# Paste your API key here. Remember to not share publicly\n",
        "api_key = 'PMQ1E5rLQXivEHEK5fGWoideOmEWrfmWRV9aTGKK'\n",
        "\n",
        "# Create and retrieve a Cohere API key from os.cohere.ai\n",
        "co = cohere.Client(api_key)"
      ],
      "metadata": {
        "id": "Xf7eCDaLElFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gemini_embeddings(txt):\n",
        "  result = client.models.embed_content(\n",
        "          model=\"embedding-001\",\n",
        "          contents=txt)\n",
        "\n",
        "  return result.embeddings"
      ],
      "metadata": {
        "id": "Hxk36lqVsVHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan.\n",
        "It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine.\n",
        "Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind.\n",
        "\n",
        "Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007.\n",
        "Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar.\n",
        "Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm.\n",
        "Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles.\n",
        "Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects.\n",
        "\n",
        "Interstellar premiered on October 26, 2014, in Los Angeles.\n",
        "In the United States, it was first released on film stock, expanding to venues using digital projectors.\n",
        "The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014.\n",
        "It received acclaim for its performances, direction, screenplay, musical score, visual effects, ambition, themes, and emotional weight.\n",
        "It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics. Since its premiere, Interstellar gained a cult following,[5] and now is regarded by many sci-fi experts as one of the best science-fiction films of all time.\n",
        "Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades\"\"\"\n",
        "\n",
        "# Split into a list of sentences\n",
        "texts = text.split('.')\n",
        "\n",
        "# Clean up to remove empty spaces and new lines\n",
        "texts = [t.strip(' \\n') for t in texts]"
      ],
      "metadata": {
        "id": "6mVnr5nArphk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Embedding the Text Chunks"
      ],
      "metadata": {
        "id": "MlRu2E6GArbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "## Gemini\n",
        "# # Get the embeddings\n",
        "# response = [gemini_embeddings(t) for t in texts]\n",
        "\n",
        "# embeds = np.array([np.array(response[i][0].values) for i in range(len(response))])\n",
        "# print(embeds.shape)\n",
        "\n",
        "# Cohere\n",
        "# Get the embeddings\n",
        "response = co.embed(\n",
        "  texts=texts,\n",
        "  input_type=\"search_document\",\n",
        ").embeddings\n",
        "\n",
        "embeds = np.array(response)\n",
        "print(embeds.shape)"
      ],
      "metadata": {
        "id": "NK_CPBzerul9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Building The Search Index"
      ],
      "metadata": {
        "id": "RAccOkUyA1FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import faiss\n",
        "# import numpy as np\n",
        "\n",
        "# # Ensure embeds is a contiguous array with float32 dtype\n",
        "# embeds = np.ascontiguousarray(embeds, dtype=np.float32)\n",
        "\n",
        "# dim = embeds.shape[1]\n",
        "# index = faiss.IndexFlatL2(dim)\n",
        "# # Reshape embeds to (number_of_sentences, embedding_dimension)\n",
        "# # index.add(embeds.astype('float32'))  # Pass the converted embeds array directly"
      ],
      "metadata": {
        "id": "4GY6NHd9Ahbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "chroma_client = chromadb.Client()\n"
      ],
      "metadata": {
        "id": "Nm4RFf3_HcEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chroma_client.delete_collection(name=\"interstellar\")"
      ],
      "metadata": {
        "id": "OwtHLwvTKBgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb.utils.embedding_functions as embedding_functions\n",
        "from datetime import datetime\n",
        "\n",
        "cohere_ef  = embedding_functions.CohereEmbeddingFunction(api_key=api_key,  model_name=\"large\")\n",
        "# cohere_ef(input=[\"document1\",\"document2\"])\n",
        "\n",
        "collection = chroma_client.create_collection(\n",
        "    name=\"interstellar\",\n",
        "    embedding_function=cohere_ef,  # Pass the wrapper function\n",
        "    metadata={\n",
        "        \"description\": \"interstellar wiki chroma collection\",\n",
        "        \"created\": str(datetime.now())\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "7RoeYSkwJrre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection.add(\n",
        "    documents=texts,\n",
        "    # embeddings=embeds.tolist(),\n",
        "    ids=[str(i) for i in range(len(texts))]\n",
        ")\n"
      ],
      "metadata": {
        "id": "fHVnPNoYIhmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Search the index"
      ],
      "metadata": {
        "id": "kVyDGhs5Na_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### cosine similarity"
      ],
      "metadata": {
        "id": "m-Uo833-NhVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = collection.query(\n",
        "    query_texts=[\"how precise was the science\"], # Chroma will embed this for you\n",
        "    n_results=3 # how many results to return\n",
        ")\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "n1Qp5pWSI3r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({'ids': list(results['ids'][0]), 'documents': list(results['documents'][0]), 'distances': list(results['distances'][0])})"
      ],
      "metadata": {
        "id": "Oox7TBWLI88t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(text):\n",
        "  results = collection.query(\n",
        "    query_texts=[text], # Chroma will embed this for you\n",
        "    n_results=3 # how many results to return\n",
        "  )\n",
        "  return pd.DataFrame({'ids': list(results['ids'][0]), 'documents': list(results['documents'][0]), 'distances': list(results['distances'][0])})\n"
      ],
      "metadata": {
        "id": "mNRs6cGQOMCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### keyword search"
      ],
      "metadata": {
        "id": "5ccI09bRNpGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "import string\n",
        "\n",
        "def bm25_tokenizer(text):\n",
        "    tokenized_doc = []\n",
        "    for token in text.lower().split():\n",
        "        token = token.strip(string.punctuation)\n",
        "\n",
        "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
        "            tokenized_doc.append(token)\n",
        "    return tokenized_doc"
      ],
      "metadata": {
        "id": "91x8XxoaNIBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "tokenized_corpus = []\n",
        "for passage in tqdm(texts):\n",
        "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
        "\n",
        "bm25 = BM25Okapi(tokenized_corpus)"
      ],
      "metadata": {
        "id": "9vgQWoFaNvdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_search(query, top_k=3, num_candidates=15):\n",
        "    print(\"Input question:\", query)\n",
        "\n",
        "    ##### BM25 search (lexical search) #####\n",
        "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
        "    top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
        "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
        "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    print(f\"Top-3 lexical search (BM25) hits\")\n",
        "    for hit in bm25_hits[0:top_k]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], texts[hit['corpus_id']].replace(\"\\n\", \" \")))"
      ],
      "metadata": {
        "id": "tMMGYShoNv0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_search(query = \"how precise was the science\")"
      ],
      "metadata": {
        "id": "qGQPLINvNymA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caveats of Dense Retrieval"
      ],
      "metadata": {
        "id": "Kfqp7ke5ODu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the mass of the moon?\"\n",
        "results = search(query)\n",
        "results"
      ],
      "metadata": {
        "id": "tULliImvN37q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reranking Example"
      ],
      "metadata": {
        "id": "EB8bPm0DUqch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"how precise was the science\"\n",
        "results = co.rerank(query=query, documents=texts, top_n=3, return_documents=True)\n",
        "results.results"
      ],
      "metadata": {
        "id": "AQJtAbSDOHZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, result in enumerate(results.results):\n",
        "    print(idx, result.relevance_score , result.document.text)"
      ],
      "metadata": {
        "id": "Ew79QLiRUsuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_and_reranking_search(query, top_k=3, num_candidates=10):\n",
        "    print(\"Input question:\", query)\n",
        "\n",
        "    ##### BM25 search (lexical search) #####\n",
        "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
        "    top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
        "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
        "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    print(f\"Top-3 lexical search (BM25) hits\")\n",
        "    for hit in bm25_hits[0:top_k]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], texts[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
        "\n",
        "    #Add re-ranking\n",
        "    docs = [texts[hit['corpus_id']] for hit in bm25_hits]\n",
        "\n",
        "    print(f\"\\nTop-3 hits by rank-API ({len(bm25_hits)} BM25 hits re-ranked)\")\n",
        "    results = co.rerank(query=query, documents=docs, top_n=top_k, return_documents=True)\n",
        "    for hit in results.results:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit.relevance_score, hit.document.text.replace(\"\\n\", \" \")))"
      ],
      "metadata": {
        "id": "-c7dX4s7U43k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_and_reranking_search(query = \"how precise was the science\")"
      ],
      "metadata": {
        "id": "00y9KVOuU5LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "v48C_RnDU9pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"income generated\"\n",
        "\n",
        "# 1- Retrieval\n",
        "# We'll use embedding search. But ideally we'd do hybrid\n",
        "results = search(query)\n",
        "\n",
        "# 2- Grounded Generation\n",
        "docs_dict = [{'documents': document} for document in results['documents']]\n",
        "response = co.chat(\n",
        "    message = query,\n",
        "    documents=docs_dict\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "T4pqwnFtU7is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.citations"
      ],
      "metadata": {
        "id": "rUTP3al2X0MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: RAG with local models"
      ],
      "metadata": {
        "id": "h-ixC-EXYx2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the local model"
      ],
      "metadata": {
        "id": "xQMvb76MY1tS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf"
      ],
      "metadata": {
        "id": "FRi3-uWOX5BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain import LlamaCpp\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=2048,\n",
        "    seed=42,\n",
        "    verbose=False,\n",
        "    # Provide the llama.cpp client\n",
        "    client=Llama(model_path=\"Phi-3-mini-4k-instruct-q4.gguf\")\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sETPVLW-Z4hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the embedding model"
      ],
      "metadata": {
        "id": "5Q8MU_XhZ-0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Embedding Model for converting text to numerical representations\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name='BAAI/bge-small-en-v1.5'\n",
        ")"
      ],
      "metadata": {
        "id": "iMZcbaJRZtug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing the vector database"
      ],
      "metadata": {
        "id": "zqgfOveXaEcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.vectorstores import FAISS\n",
        "\n",
        "# # Create a local vector database\n",
        "# db = FAISS.from_texts(texts, embedding_model)"
      ],
      "metadata": {
        "id": "An6M_YVEaBcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -qU \"langchain-chroma>=0.1.2\""
      ],
      "metadata": {
        "id": "neXu-Vhfeoy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate embedding function for chroma vector store\n",
        "\n",
        "import chromadb\n",
        "import chromadb.utils.embedding_functions as embedding_functions\n",
        "\n",
        "def create_embedding_function(api_key):\n",
        "    \"\"\"Creates a Cohere embedding function for Chroma.\"\"\"\n",
        "    cohere_ef = embedding_functions.CohereEmbeddingFunction(api_key=api_key, model_name=\"large\")\n",
        "    return cohere_ef\n"
      ],
      "metadata": {
        "id": "VnX8HYHDi44a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to get the Cohere embedding function\n",
        "embedding_function_instance = create_embedding_function(api_key=api_key)"
      ],
      "metadata": {
        "id": "iUkroPC5j2eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"example_collection\",\n",
        "    embedding_function=embedding_function_instance,\n",
        "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
        ")"
      ],
      "metadata": {
        "id": "jF-sH0gzefCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The RAG prompt"
      ],
      "metadata": {
        "id": "F4sS8CwRaQRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "# Create a prompt template\n",
        "template = \"\"\"<|user|>\n",
        "Relevant information:\n",
        "{context}\n",
        "\n",
        "Provide a concise answer the following question using the relevant information provided above:\n",
        "{question}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# RAG Pipeline\n",
        "rag = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type='stuff',\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    chain_type_kwargs={\n",
        "        \"prompt\": prompt\n",
        "    },\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "5iwhI_nuaIHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "rag.invoke('Income generated')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "TjEOsMx1aaDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Create a prompt template\n",
        "# Removing the \"<|user|>\" and \"<|assistant|>\" tags\n",
        "template = \"\"\"Relevant information:\n",
        "{context}\n",
        "\n",
        "Provide a concise answer the following question using the relevant information provided above:\n",
        "{question}\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# RAG Pipeline\n",
        "rag = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type='stuff',\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    chain_type_kwargs={\n",
        "        \"prompt\": prompt\n",
        "    },\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "#Invoking the RAG pipeline\n",
        "result = rag(\"Income generated\") #Use call method instead of invoke to avoid suffix parameter.\n",
        "print(result['result']) #Print the generated answer\n"
      ],
      "metadata": {
        "id": "lkk6j1SfaSkH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}